{"filename": "data/unstructured/raw/Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf", "content": [{"page_number": 1, "page_content": "Generated Knowledge Prompting for Commonsense Reasoning Jiacheng Liu\u2665 Alisa Liu\u2665 Ximing Lu\u2665\u2660 Sean Welleck\u2665\u2660 Peter West\u2665\u2660 Ronan Le Bras\u2660 Yejin Choi\u2665\u2660 Hannaneh Hajishirzi\u2665\u2660 \u2665Paul G. Allen School of Computer Science & Engineering, University of Washington \u2660Allen Institute for Artificial Intelligence liujc@cs.washington.edu arXiv:2110.08387v3 [cs.CL] 28 Sep 2022 Abstract It remains an open question whether incorpo- rating external knowledge benefits common- sense reasoning while maintaining the flexi- bility of pretrained sequence models. To in- vestigate this question, we develop generated knowledge prompting, which consists of gen- erating knowledge from a language model, then providing the knowledge as additional in- put when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a struc- tured knowledge base, yet it improves perfor- mance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achiev- ing state-of-the-art results on numerical com- monsense (NumerSense), general common- sense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Gener- ated knowledge prompting highlights large- scale language models as flexible sources of external knowledge for improving common- sense reasoning. Our code is available at github.com/liujch1998/GKP 1 Introduction It remains an open research question whether exter- nal knowledge is needed for commonsense reason- ing. On one hand, a substantial body of prior work has reported that integrating external knowledge can help improve task performance (Mitra et al., 2019; Bian et al., 2021, inter alia), especially if the knowledge is high quality (e.g. hand-crafted by ex- perts). On the other hand, recent leaderboards are often dominated by large-scale pretrained models that are fine-tuned on a target benchmark (Khashabi et al., 2020; Lourie et al., 2021), suggesting that the benefits of external knowledge may wash away as the underlying models increase in size and are pretrained on ever larger amounts of raw text. Even if external knowledge is found to be ef- fective on a particular task, flexibility remains a fundamental hurdle to integrating external knowl- Figure 1: Generated knowledge prompting involves (i) using few-shot demonstrations to generate question- related knowledge statements from a language model; (ii) using a second language model to make predic- tions with each knowledge statement, then selecting the highest-confidence prediction. edge, as many benchmarks currently lack appropri- ate knowledge bases with sufficient coverage. Fur- thermore, prior methods often require task-specific, custom supervision for knowledge integration (Mi- tra et al., 2019; Chang et al., 2020), introducing a burden for rapidly adapting new pretrained models to a wide variety of tasks. In this paper, we investigate whether external knowledge can be helpful for commonsense rea- soning, even on top of the largest state-of-the-art pretrained models (e.g. T5-11b (Raffel et al., 2019) and its variants), with a focus on four recent com- monsense benchmarks. To facilitate easier adap- tation with any zero-shot or finetuned models, we propose an approach that does not require access to a structured knowledge base or joint finetuning for knowledge integration. The key insight behind our method, Generated Knowledge Prompting (sketched in Figure 1), is that we can generate useful knowledge from a lan- guage model, then provide the knowledge as an in- put prompt that is concatenated with a question. To Knowledge 1 Knowledge Integration Question Knowledge Generation Knowledge 2 Answer ... Prompt Instruction Generate by sampling Knowledge 1 Q(1), K(1) \u00bfDemonstrations: (fixed for task) Q(5), K(5) mmmm PLM Knowledge 2 ... Question"}, {"page_number": 2, "page_content": "Dataset Question / Knowledge Prediction Score NumerSense the word children means [M] or more kids. The word child means one kid. one two 0.37 | 0.35 0.91 CSQA She was always helping at the senior center, it brought her what? People who help others are usually happier. feel better happiness 0.97 | 0.02 0.98 CSQA2 Part of golf is trying to get a higher point total than others. The player with the lowest score wins. yes no 1.00 | 0.00 1.00 QASC Sponges eat primarily Sponges eat bacteria and other tiny organisms. cartilage krill and plankton 0.95 | 0.00 0.99 Table 1: Examples where prompting with generated knowledge rectifies model prediction. Each section shows the correct answer in green, the incorrect answer in red, and the prediction scores from the inference model that only sees the question (top) and the same model that sees the question prompted with the given knowledge (bottom). support a variety of settings without finetuning, the quality and flexibility of knowledge is crucial. We propose a simple, yet effective, method that elicits knowledge statements (i.e. knowledge expressed as natural language statements) from generic lan- guage models in a few-shot setting. Compared to prior work that elicits knowledge via clarification questions (Shwartz et al., 2020) or contrastive ex- planations (Paranjape et al., 2021), our approach can generate knowledge flexibly, beyond the scope of pre-defined templates (Table 1). Experiments show that our method improves both zero-shot and finetuned models on numeri- cal commonsense (NumerSense (Lin et al., 2020)), general commonsense (CommonsenseQA (Talmor et al., 2019), CommonsenseQA 2.0 (Talmor et al., 2021)), and scientific commonsense (QASC (Khot et al., 2020)) benchmarks, setting a new state-of- the-art on three of these datasets. It outperforms the template-based knowledge generation method self-talk (Shwartz et al., 2020), while performing comparably to retrieval-based systems. We find three factors contribute to the perfor- mance of generated knowledge prompting: (i) the quality of knowledge, (ii) the quantity of knowl- edge where the performance improves with more knowledge statements, and (iii) the strategy for integrating knowledge during inference. Our quali- tative analysis suggests that the generated knowl- edge statements cover a variety of types, and can transform commonsense question answering to ex- plicit reasoning procedures, e.g. deduction, that are supported by off-the-shelf and finetuned language models. 2 Generated Knowledge Prompting A multiple-choice commonsense reasoning task involves predicting an answer a \u2208 Aq given a ques- tion q \u2208 Q, where the set of choices Aq is finite and can vary by question, and both questions and answers are variable-length text sequences. Our method answers commonsense questions in two steps. The first step is knowledge generation, where we use a language model pG(k|q) to generate knowl- edge statements conditioned on the question: Kq = {km : km \u223c pG(k|q), m = 1 . . . M}, where each knowledge statement km is a variable- length text sequence. Intuitively, each statement contains information that is helpful for answering the question (e.g. Table 1). The second step is knowledge integration, where generated knowledge is integrated into the decision process of a language model used for inference, a\u02c6 = arg max pI (a|q, Kq). a\u2208Aq In contrast, the vanilla setting of using the infer- ence model without knowledge is represented by a\u02c6 = arg maxa\u2208Aq pI (a|q). Next, we describe the knowledge generation and integration steps in detail. 2.1 Knowledge Generation We generate question-related knowledge state- ments by prompting a language model. The prompt consists of an instruction, a few demonstrations that are fixed for each task, and a new-question place- holder. The demonstrations are human-written, and each consists of a question in the style of the task and a knowledge statement that is helpful for an- swering this question. For a given task, we write five demonstrations using the format in Table 2. We write questions (or select them from the train- ing set, when available) that are representative of"}]}