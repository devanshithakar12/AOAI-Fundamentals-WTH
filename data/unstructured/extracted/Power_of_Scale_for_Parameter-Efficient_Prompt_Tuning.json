{"filename": "data/unstructured/raw/Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf", "content": [{"page_number": 1, "page_content": "The Power of Scale for Parameter-Efficient Prompt Tuning Brian Lester\u2217 Rami Al-Rfou Noah Constant Google Research {brianlester,rmyeid,nconstant}@google.com arXiv:2104.08691v2 [cs.CL] 2 Sep 2021 Abstract In this work, we explore \u201cprompt tuning,\u201d a simple yet effective mechanism for learn- ing \u201csoft prompts\u201d to condition frozen lan- guage models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through back- propagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3\u2019s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning be- comes more competitive with scale: as mod- els exceed billions of parameters, our method \u201ccloses the gap\u201d and matches the strong per- formance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as Model Tuning Model Tuning (Multi-task) 100 Prompt Design -x- Prompt Tuning 90 80 SuperGLUE Score a simplification of the recently proposed \u201cprefix tuning\u201d of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that condition- ing a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \u201cprompt ensembling.\u201d 70 60 50 108 109 1010 Model Parameters 1011 Figure 1: Standard model tuning of T5 achieves strong performance, but requires storing separate copies of the model for each end task. Our prompt tuning of T5 matches the quality of model tuning as size increases, while enabling the reuse of a single frozen model for all tasks. Our approach significantly outperforms few- shot prompt design using GPT-3. We show mean and standard deviation across 3 runs for tuning methods. 1 Introduction With the wide success of pre-trained large lan- guage models, a range of techniques has arisen to adapt these general-purpose models to downstream tasks. ELMo (Peters et al., 2018) proposed freezing the pre-trained model and learning a task-specific weighting of its per-layer representations. How- ever, since GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), the dominant adaptation tech- nique has been model tuning (or \u201cfine-tuning\u201d), where all model parameters are tuned during adap- tation, as proposed by Howard and Ruder (2018). More recently, Brown et al. (2020) showed that prompt design (or \u201cpriming\u201d) is surprisingly effec- tive at modulating a frozen GPT-3 model\u2019s behavior through text prompts. Prompts are typically com- posed of a task description and/or several canonical examples. This return to \u201cfreezing\u201d pre-trained models is appealing, especially as model size con- tinues to increase. Rather than requiring a separate copy of the model for each downstream task, a single generalist model can simultaneously serve many different tasks. \u2217Work done as a Google AI Resident. Unfortunately, prompt-based adaptation has sev- eral key drawbacks. Task description is error-prone and requires human involvement, and the effective- ness of a prompt is limited by how much condition- ing text can fit into the model\u2019s input. As a result, downstream task quality still lags far behind that of tuned models. For instance, GPT-3 175B few- shot performance on SuperGLUE is 17.5 points be-"}, {"page_number": 2, "page_content": "Model Tuning Pre-trained Model (11B params) Prompt Tuning a1 a2 Task A Batch b1 Task B Batch c1 Task C c2 Batch Mixed-task Batch Task A Model (11B params) A a1 A C c1 b1 a2 B A B Task B Model (11B params) -- C c2 C Task Prompts (20K params each) Task C Model (11B params) ---- Pre-trained Model (11B params) Figure 2: Model tuning requires making a task- specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires stor- ing a small task-specific prompt for each task, and enables mixed-task inference using the original pre- trained model. With a T5 \u201cXXL\u201d model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task\u2014a reduction of over five orders of magnitude\u2014assuming a prompt length of 5 tokens. low fine-tuned T5-XXL (Raffel et al., 2020) (71.8 vs. 89.3) despite using 16 times more parameters. Several efforts to automate prompt design have been recently proposed. Shin et al. (2020) propose a search algorithm over the discrete space of words, guided by the downstream application training data. While this technique outperforms manual prompt design, there is still a gap relative to model tuning. Li and Liang (2021) propose \u201cprefix tuning\u201d and show strong results on generative tasks. This method freezes the model parameters and back- propagates the error during tuning to prefix ac- tivations prepended to each layer in the encoder stack, including the input layer. Hambardzumyan et al. (2021) simplify this recipe by restricting the trainable parameters to the input and output sub- networks of a masked language model, and show reasonable results on classifications tasks. In this paper, we propose prompt tuning as a further simplification for adapting language models. We freeze the entire pre-trained model and only al- low an additional k tunable tokens per downstream task to be prepended to the input text. This \u201csoft prompt\u201d is trained end-to-end and can condense the signal from a full labeled dataset, allowing our method to outperform few-shot prompts and close the quality gap with model tuning (Figure 1). At the same time, since a single pre-trained model is recycled for all downstream tasks, we retain the ef- ficient serving benefits of frozen models (Figure 2). While we developed our method concurrently with Li and Liang (2021) and Hambardzumyan et al. (2021), we are the first to show that prompt tuning alone (with no intermediate-layer prefixes or task-specific output layers) is sufficient to be com- petitive with model tuning. Through detailed ex- periments in sections 2\u20133, we demonstrate that lan- guage model capacity is a key ingredient for these approaches to succeed. As Figure 1 shows, prompt tuning becomes more competitive with scale. We compare with similar approaches in Sec- tion 4. Explicitly separating task-specific param- eters from the \u201cgeneralist\u201d parameters needed for general language-understanding has a range of ad- ditional benefits. We show in Section 5 that by capturing the task definition in the prompt while keeping the generalist parameters fixed, we are able to achieve better resilience to domain shifts. In Sec- tion 6, we show that \u201cprompt ensembling\u201d, learn- ing multiple prompts for the same task, can boost quality and is more efficient than classic model en- sembling. Finally, in Section 7, we investigate the interpretability of our learned soft prompts. In sum, our key contributions are: 1. Proposing prompt tuning and showing its com- petitiveness with model tuning in the regime of large language models. 2. Ablating many design choices, and showing quality and robustness improve with scale. 3. Showing prompt tuning outperforms model tuning on domain shift problems. 4. Proposing \u201cprompt ensembling\u201d and showing its effectiveness. 2 Prompt Tuning Following the \u201ctext-to-text\u201d approach of T5 (Raffel et al., 2020), we cast all tasks as text generation. Instead of modeling classification as the probabil- ity of an output class given some input, Pr(y|X), where X is a series of tokens and y is a single class label, we now model it as conditional generation, where Y is a sequence of tokens that represent a class label. T5 models classification as Pr\u03b8(Y |X), parameterized by the weights, \u03b8, of the transform- ers (Vaswani et al., 2017) that make up its encoder and decoder. Prompting is the approach of adding extra in- formation for the model to condition on during its generation of Y . Normally, prompting is done by prepending a series of tokens, P, to the in- put X, such that the model maximizes the likeli- hood of the correct Y , Pr\u03b8(Y |[P; X]), while keep-"}]}