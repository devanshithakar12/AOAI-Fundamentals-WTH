{"filename": "data/unstructured/raw/Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf", "content": [{"page_number": 1, "page_content": "Prefix-Tuning: Optimizing Continuous Prompts for Generation arXiv:2101.00190v1 [cs.CL] 1 Jan 2021 Xiang Lisa Li Stanford University xlisali@stanford.edu Abstract Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for nat- ural language generation tasks, which keeps language model parameters frozen, but opti- mizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspira- tion from prompting, allowing subsequent to- kens to attend to this prefix as if it were \u201cvir- tual tokens\u201d. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data set- ting, outperforms fine-tuning in low-data set- tings, and extrapolates better to examples with topics unseen during training. 1 Introduction Fine-tuning is the prevalent paradigm for using large pretrained language models (LMs) (Radford et al., 2019; Devlin et al., 2019) to perform down- stream tasks (e.g., summarization), but it requires updating and storing all the parameters of the LM. Consequently, to build and deploy NLP systems that rely on large pretrained LMs, one currently needs to store a modified copy of the LM parame- ters for each task. This can be prohibitively expen- sive, given the large size of current LMs; for exam- ple, GPT-2 has 774M parameters (Radford et al., 2019) and GPT-3 has 175B parameters (Brown et al., 2020). A natural approach to this problem is lightweight fine-tuning, which freezes most of the pretrained parameters and augments the model with small trainable modules. For example, adapter-tuning Percy Liang Stanford University pliang@cs.stanford.edu Figure 1: Fine-tuning (top) updates all Transformer parameters (the red Transformer box) and requires stor- ing a full model copy for each task. We propose prefix-tuning (bottom), which freezes the Transformer parameters and only optimizes the prefix (the red pre- fix blocks). Consequently, we only need to store the prefix for each task, making prefix-tuning modular and space-efficient. Note that each vertical block denote transformer activations at one time step. (Rebuffi et al., 2017; Houlsby et al., 2019) inserts additional task-specific layers between the layers of pretrained language models. Adapter-tuning has promising performance on natural language understanding and generation benchmarks, attain- ing comparable performance with fine-tuning while adding only around 2-4% task-specific parameters (Houlsby et al., 2019; Lin et al., 2020). On the extreme end, GPT-3 (Brown et al., 2020) can be deployed without any task-specific tuning. Instead, users prepend a natural language task in- struction (e.g., TL;DR for summarization) and a few examples to the task input; then generate the output from the LM. This approach is known as in-context learning or prompting. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural lan- guage generation (NLG) tasks, inspired by prompt- ing. Consider the task of generating a textual de- Fine-tuning Transformer (Translation) Transformer (Summarization) Transformer (Table-to-text) name Starbucks type coffee shop [SEP] Starbucks serves coffee Input (table-to-text) Prefix (Translation) Output (table-to-text) Prefix-tuning Prefix (Summarization) Prefix (Table-to-text) Transformer (Pretrained) name Starbucks type coffee shop [SEP] Starbucks serves coffee Input (table-to-text) Output (table-to-text)"}, {"page_number": 2, "page_content": "scription of a data table, as shown in Figure 1, where the task input is a linearized table (e.g., \u201cname: Starbucks | type: coffee shop\u201d) and the out- put is a textual description (e.g., \u201cStarbucks serves coffee.\u201d). Prefix-tuning prepends a sequence of continuous task-specific vectors to the input, which we call a prefix, depicted by red blocks in Figure 1 (bottom). For subsequent tokens, the Transformer can attend to the prefix as if it were a sequence of \u201cvirtual tokens\u201d, but unlike prompting, the prefix consists entirely of free parameters which do not correspond to real tokens. In contrast to fine-tuning in Figure 1 (top), which updates all Transformer parameters and thus requires storing a tuned copy of the model for each task, prefix-tuning only op- timizes the prefix. Consequently, we only need to store one copy of the large Transformer and a learned task-specific prefix, yielding a very small overhead for each additional task (e.g., 250K pa- rameters for table-to-text). In contrast to fine-tuning, prefix-tuning is mod- ular: we train an upstream prefix which steers a downstream LM, which remains unmodified. Thus, a single LM can support many tasks at once. In the context of personalization where the tasks cor- respond to different users (Shokri and Shmatikov, 2015; McMahan et al., 2016), we could have a sep- arate prefix for each user trained only on that user\u2019s data, thereby avoiding data cross-contamination. Moreover, the prefix-based architecture enables us to even process examples from multiple users/tasks in a single batch, something that is not possible with other lightweight fine-tuning approaches. We evaluate prefix-tuning on table-to-text gen- eration using GPT-2 and abstractive summariza- tion using BART. In terms of storage, prefix-tuning stores 1000x fewer parameters than fine-tuning. In terms of performance when trained on full datasets, prefix-tuning and fine-tuning are comparable for table-to-text (\u00a76.1), while prefix-tuning suffers a small degradation for summarization (\u00a76.2). In low- data settings, prefix-tuning on average outperforms fine-tuning on both tasks (\u00a76.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (\u00a76.4). 2 Related Work Fine-tuning for natural language generation. Current state-of-the-art systems for natural lan- guage generation are based on fine-tuning pre- trained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstrac- tive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020) respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine transla- tion and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and summarization using BART, but prefix-tuning can be applied to other generation tasks and pre- trained models. Lightweight fine-tuning. Lightweight fine- tuning freezes most of the pretrained parameters and modifies the pretrained model with small trainable modules. The key challenge is to identify high-performing architectures of the modules and the subset of pretrained parameters to tune. One line of research considers removing parameters: some model weights are ablated away by training a binary mask over model parameters (Zhao et al., 2020; Radiya-Dixit and Wang, 2020). Another line of research considers inserting parameters. For example, Zhang et al. (2020a) trains a \u201cside\u201d network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific lay- ers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020). Compared to this line of work, which tunes around 3.6% of the LM parameters, our method obtains a further 30x reduction in task-specific parameters, tuning only 0.1% while maintaining comparable performance. Prompting. Prompting means prepending in- structions and a few examples to the task input and generating the output from the LM. GPT-3 (Brown et al., 2020) uses manually designed prompts to adapt its generation for different tasks, and this framework is termed in-context learning. However, since Transformers can only condition on a bounded-length context (e.g., 2048 tokens for GPT- 3), in-context learning is unable to fully exploit training sets longer than the context window. Sun and Lai (2020) also prompt by keywords to control for sentiment or topic of the generated sentence. In natural language understanding tasks, prompt"}]}