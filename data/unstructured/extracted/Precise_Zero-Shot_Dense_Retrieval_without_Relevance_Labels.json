{"filename": "data/unstructured/raw/Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf", "content": [{"page_number": 1, "page_content": "Precise Zero-Shot Dense Retrieval without Relevance Labels Luyu Gao\u2217 \u2020 Xueguang Ma\u2217 \u2021 Jimmy Lin\u2021 Jamie Callan\u2020 \u2020Language Technologies Institute, Carnegie Mellon University \u2021David R. Cheriton School of Computer Science, University of Waterloo {luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca arXiv:2212.10496v1 [cs.IR] 20 Dec 2022 Abstract While dense retrieval has been shown effec- tive and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no rel- evance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we pro- pose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to gen- erate a hypothetical document. The docu- ment captures relevance patterns but is unreal and may contain false details. Then, an un- supervised contrastively learned encoder (e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder\u2019s dense bottleneck filtering out the incorrect details. Our exper- iments show that HyDE significantly outper- forms the state-of-the-art unsupervised dense retriever Contriever and shows strong per- formance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages (e.g. sw, ko, ja).1 \u4e00 1 Introduction Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification. A variety of meth- ods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofst\u00e4tter et al., 2021) and task-specific \u2217 Equal contribution. 1No models were trained or fine-tuned in making this pre- print. Our open source code is available at https://github. com/texttron/hyde. pre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models. On the other hand, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios. In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned"}, {"page_number": 2, "page_content": "write a passage to answer the question how long does it take to remove wisdom tooth HyDE It usually takes between 30 minutes and two hours to remove a wisdom tooth... write a scientific paper passage to answer the question How has the COVID-19 pandemic impacted mental health? How wisdom teeth are removed... Some ... a few minutes, whereas others can take 20 minutes or longer.... ...depression and anxiety had A increased by 20% since the start of the pandemic... GPT Contriever ... two studies investigating COVID-19 patients ... significantly higher level of depressive ... \uc778 \uac04 \uc774 \ubd88 \uc744 \uc0ac \uc6a9 \ud55c \uae30 \ub85d \uc740 \uc57d 800 \ub9cc \ub144 \uc804 \ubd80 \ud130 \ub098 \ud0c0 \ub09c \ub2e4 ... write a passage in Korean to answer the question in detail \uc778 \uac04 \uc740 \uc5b8 \uc81c \ubd88 \uc744 \uc0ac \uc6a9 \ud588 \ub294 \uac00 ? instruction query ... \ubd88 \uc744 \ucc98 \uc74c \uc0ac \uc6a9 \ud55c \uc2dc \uae30 \ub294 \ud638 \ubaa8 \uc5d0 \ub809 \ud22c \uc2a4 \uac00 \uc0b4 \uc558 \ub358 142 \ub9cc \ub144 \uc804 \uc73c \ub85c \uac70 \uc2ac \ub7ec \uac04 \ub2e4 ... generated document real document Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever/mContriever models. to human intent to follow instructions. With these ingredients, we propose through Hypothetical Document Embeddings (HyDE), and decompose dense retrieval into two tasks, a generative task per- formed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. - to pivot We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder\u2019s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks. HyDE appears unsupervised. No model is trained in HyDE: both the generative model and the con- trastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM. In our experiments, we show HyDE using Instruct- GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly out- performs the previous state-of-the-art Contriever- only zero-shot no-relevance system on 11 queries sets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese. 2 Related Works Dense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers stud- ied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sam- pling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; Hofst\u00e4tter et al., 2021). Later works studied the second stage pre-training of language model specifically for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022). The popularity of dense retrieval can be partially attributed to the rich and successful research in very efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017). Instructions-Following Language Models Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied \u201cTask-aware Retrieval with Instructions\u201d. They fine-tuned dense encoders that can also encode task-specific instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an"}]}